{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lacey\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from numpy.random import shuffle\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "from gensim import corpora\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When I want a sandwich, this is where I want i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't think I've ever had a good experience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretty standard Chinese food.  Never had a bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This little diner is a staple in Bloomfield's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We've been to this location a few times but I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  When I want a sandwich, this is where I want i...\n",
       "1  I don't think I've ever had a good experience ...\n",
       "2  Pretty standard Chinese food.  Never had a bad...\n",
       "3  This little diner is a staple in Bloomfield's ...\n",
       "4  We've been to this location a few times but I'..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load reviews for wordvector training\n",
    "train=pd.read_csv(\"wordvector_train.csv\",header=0,delimiter=\"\\t\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>duHFBe87uNSXImQmvBh87Q</td>\n",
       "      <td>When I want a sandwich, this is where I want i...</td>\n",
       "      <td>['Sandwiches', 'Restaurants']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDMRxmcKPNt1AHPBKqO64Q</td>\n",
       "      <td>I don't think I've ever had a good experience ...</td>\n",
       "      <td>['Burgers', 'Bars', 'Restaurants', 'Sports Bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iFEiMJoEqyB9O8OUNSdLzA</td>\n",
       "      <td>Pretty standard Chinese food.  Never had a bad...</td>\n",
       "      <td>['Chinese', 'Restaurants']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HmI9nhgOkrXlUr6KZGZZew</td>\n",
       "      <td>This little diner is a staple in Bloomfield's ...</td>\n",
       "      <td>['Sandwiches', 'Restaurants', 'Italian', 'Dine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qnpvw-uQyRn9nlClWFK9aA</td>\n",
       "      <td>We've been to this location a few times but I'...</td>\n",
       "      <td>['Chicken Wings', 'Restaurants']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business id                                            reviews  \\\n",
       "0  duHFBe87uNSXImQmvBh87Q  When I want a sandwich, this is where I want i...   \n",
       "1  SDMRxmcKPNt1AHPBKqO64Q  I don't think I've ever had a good experience ...   \n",
       "2  iFEiMJoEqyB9O8OUNSdLzA  Pretty standard Chinese food.  Never had a bad...   \n",
       "3  HmI9nhgOkrXlUr6KZGZZew  This little diner is a staple in Bloomfield's ...   \n",
       "4  qnpvw-uQyRn9nlClWFK9aA  We've been to this location a few times but I'...   \n",
       "\n",
       "                                          categories  \n",
       "0                      ['Sandwiches', 'Restaurants']  \n",
       "1  ['Burgers', 'Bars', 'Restaurants', 'Sports Bar...  \n",
       "2                         ['Chinese', 'Restaurants']  \n",
       "3  ['Sandwiches', 'Restaurants', 'Italian', 'Dine...  \n",
       "4                   ['Chicken Wings', 'Restaurants']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load parsed dataset for aspect and sentiment prediction\n",
    "predict=pd.read_csv(\"reviews+categories.csv\",header=0,delimiter=\",\")\n",
    "predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>refused to refund me</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Really a beautiful spot</td>\n",
       "      <td>ambience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But the worst part(s): the lettuce on my sand...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Service is friendly and courteous</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The pizza was really cold and Soggy so my gue...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews   Aspects\n",
       "0                               refused to refund me   service\n",
       "1                            Really a beautiful spot  ambience\n",
       "2   But the worst part(s): the lettuce on my sand...      food\n",
       "3                  Service is friendly and courteous   service\n",
       "4   The pizza was really cold and Soggy so my gue...      food"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load labelled aspect reviews for training \n",
    "aspect_l=pd.read_csv(\"labelled1.csv\", header=0, delimiter=\",\")\n",
    "#shuffle data\n",
    "aspect_l = aspect_l.sample(frac=1).reset_index(drop=True)\n",
    "aspect_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very cool neighborhood charming \"dive\" bar wit...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neat and tidy, good and quick customer servic...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before I get into the food, the service at Cuc...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a deli that does stand out among the ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We waited nearly 40 minutes for our food</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews sentiment\n",
       "0  Very cool neighborhood charming \"dive\" bar wit...  Positive\n",
       "1   Neat and tidy, good and quick customer servic...  Positive\n",
       "2  Before I get into the food, the service at Cuc...  Positive\n",
       "3   This is a deli that does stand out among the ...  Positive\n",
       "4           We waited nearly 40 minutes for our food  Negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load labelled aspect reviews for training \n",
    "sentiment_l=pd.read_csv(\"labelled3.csv\", header=0, delimiter=\",\")\n",
    "#shuffle data\n",
    "sentiment_l = sentiment_l.sample(frac=1).reset_index(drop=True)\n",
    "sentiment_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample label of labelled aspects\n",
      "[[0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]]\n",
      "Sample label of labelled sentiments\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "#convert labels into the form of \"list of list\"\n",
    "labels1=[str(item[\"Aspects\"]).split(\",\") for index, item in aspect_l.iterrows()]\n",
    "labels2=[str(item[\"sentiment\"]).split(\",\") for index, item in sentiment_l.iterrows()]\n",
    "#create indicator matrix for labels\n",
    "mlb1=MultiLabelBinarizer()\n",
    "mlb2=MultiLabelBinarizer()\n",
    "Y1=mlb1.fit_transform(labels1)\n",
    "Y2=mlb2.fit_transform(labels2)\n",
    "print(\"Sample label of labelled aspects\")\n",
    "print(Y1[0:5])\n",
    "print(\"Sample label of labelled sentiments\")\n",
    "print(Y2[0:5])\n",
    "#mlb1.classes_\n",
    "#mlb2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['When', 'want', 'sandwich', 'this', 'is', 'where', 'want', 'it', 'from', 'Soft', 'flavorful', 'bread', 'you', 'are', 'already', 'ahead', 'of', 'the', 'game', 'because', 'Subway', 'and', 'Jimmy', 'John', 'have', 'hard', 'and/or', 'bland', 'bread', 'Fresh', 'sliced', 'meats', 'cheeses', 'and', 'veggies', 'sorry', 'but', 'JJ', 'have', 'slimy', 'meat', 'and', 'that', 'HUGE', 'turn-off', 'and', 'when', 'you', 'take', 'bite', 'of', 'your', 'sandwich', 'and', 'all', 'of', 'the', 'mayo/mustard/vinegar/oregano', 'have', 'magically', 'melded', 'together', 'you', 'll', 'be', 'believer', 'too', 'Prices', 'are', 'slightly', 'higher', 'than', 'Subway', 'pretty', 'similar', 'to', 'Jimmy', 'John', 'but', 'the', 'quality', 'leaves', 'them', 'in', 'the', 'dust', 'My', 'only', 'suggestion', 'would', 'be', 'for', 'them', 'to', 'start', 'delivery', 'service', 'or', 'subscribe', 'to', 'Grubhub.com', 'or', 'something', 'like', 'that', 'so', 'could', 'order', 'it', 'for', 'our', 'lunch', 'meetings', 'Oh', 'and', 'the', 'service', 'is', 'always', 'done', 'with', 'smile', 'Great', 'job', 'guys'], ['do', \"n't\", 'think', 've', 'ever', 'had', 'good', 'experience', 'here', 'They', 'either', 'get', 'my', 'order', 'wrong', 'and/or', 'the', 'food', 'is', 'disgusting']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize reviews in wordvector_train.csv and prepare for word vector training\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in train['reviews']]\n",
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-22 13:03:03,710 : INFO : collecting all words and their counts\n",
      "2018-04-22 13:03:03,715 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-04-22 13:03:04,273 : INFO : PROGRESS: at sentence #10000, processed 1003962 words, keeping 41079 word types\n",
      "2018-04-22 13:03:04,840 : INFO : PROGRESS: at sentence #20000, processed 1996168 words, keeping 60409 word types\n",
      "2018-04-22 13:03:05,346 : INFO : PROGRESS: at sentence #30000, processed 2980257 words, keeping 75720 word types\n",
      "2018-04-22 13:03:05,817 : INFO : PROGRESS: at sentence #40000, processed 3989289 words, keeping 89728 word types\n",
      "2018-04-22 13:03:05,867 : INFO : collected 90545 word types from a corpus of 4061122 raw words and 41090 sentences\n",
      "2018-04-22 13:03:05,870 : INFO : Loading a fresh vocabulary\n",
      "2018-04-22 13:03:06,055 : INFO : min_count=5 retains 21421 unique words (23% of original 90545, drops 69124)\n",
      "2018-04-22 13:03:06,055 : INFO : min_count=5 leaves 3958990 word corpus (97% of original 4061122, drops 102132)\n",
      "2018-04-22 13:03:06,226 : INFO : deleting the raw counts dictionary of 90545 items\n",
      "2018-04-22 13:03:06,235 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2018-04-22 13:03:06,239 : INFO : downsampling leaves estimated 3120690 word corpus (78.8% of prior 3958990)\n",
      "2018-04-22 13:03:06,243 : INFO : estimated required memory for 21421 words and 200 dimensions: 44984100 bytes\n",
      "2018-04-22 13:03:06,408 : INFO : resetting layer weights\n",
      "2018-04-22 13:03:07,321 : INFO : training model with 4 workers on 21421 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-04-22 13:03:08,355 : INFO : PROGRESS: at 2.51% examples, 392962 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:09,420 : INFO : PROGRESS: at 5.49% examples, 415036 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:10,423 : INFO : PROGRESS: at 8.35% examples, 425179 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:11,425 : INFO : PROGRESS: at 11.62% examples, 445533 words/s, in_qsize 8, out_qsize 1\n",
      "2018-04-22 13:03:12,431 : INFO : PROGRESS: at 14.94% examples, 459497 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:13,431 : INFO : PROGRESS: at 18.14% examples, 467668 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:14,467 : INFO : PROGRESS: at 21.65% examples, 474252 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:15,492 : INFO : PROGRESS: at 24.76% examples, 474416 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:16,493 : INFO : PROGRESS: at 27.36% examples, 467373 words/s, in_qsize 8, out_qsize 1\n",
      "2018-04-22 13:03:17,501 : INFO : PROGRESS: at 30.38% examples, 467513 words/s, in_qsize 6, out_qsize 1\n",
      "2018-04-22 13:03:18,509 : INFO : PROGRESS: at 33.43% examples, 467558 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:19,522 : INFO : PROGRESS: at 35.82% examples, 459436 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:20,565 : INFO : PROGRESS: at 37.65% examples, 445609 words/s, in_qsize 5, out_qsize 2\n",
      "2018-04-22 13:03:21,607 : INFO : PROGRESS: at 39.58% examples, 434344 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:22,616 : INFO : PROGRESS: at 42.08% examples, 430071 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:23,676 : INFO : PROGRESS: at 44.76% examples, 427812 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:24,689 : INFO : PROGRESS: at 47.16% examples, 424763 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:25,718 : INFO : PROGRESS: at 49.91% examples, 424185 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:26,721 : INFO : PROGRESS: at 53.04% examples, 427335 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:27,735 : INFO : PROGRESS: at 55.91% examples, 428171 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:28,752 : INFO : PROGRESS: at 59.05% examples, 431284 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:29,757 : INFO : PROGRESS: at 62.18% examples, 433006 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:30,767 : INFO : PROGRESS: at 64.53% examples, 429897 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:31,801 : INFO : PROGRESS: at 66.98% examples, 427573 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:32,822 : INFO : PROGRESS: at 69.47% examples, 425714 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:33,838 : INFO : PROGRESS: at 71.73% examples, 422612 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:34,878 : INFO : PROGRESS: at 74.23% examples, 420791 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:35,891 : INFO : PROGRESS: at 76.34% examples, 417601 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:36,899 : INFO : PROGRESS: at 78.19% examples, 413374 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:37,913 : INFO : PROGRESS: at 80.19% examples, 409143 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:38,955 : INFO : PROGRESS: at 82.18% examples, 405750 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:39,978 : INFO : PROGRESS: at 84.48% examples, 403998 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:40,994 : INFO : PROGRESS: at 86.88% examples, 403115 words/s, in_qsize 7, out_qsize 0\n",
      "2018-04-22 13:03:42,015 : INFO : PROGRESS: at 89.27% examples, 401962 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:43,025 : INFO : PROGRESS: at 91.77% examples, 401487 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:44,035 : INFO : PROGRESS: at 94.28% examples, 401042 words/s, in_qsize 6, out_qsize 1\n",
      "2018-04-22 13:03:45,053 : INFO : PROGRESS: at 96.72% examples, 400537 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:46,079 : INFO : PROGRESS: at 99.25% examples, 400299 words/s, in_qsize 8, out_qsize 0\n",
      "2018-04-22 13:03:46,268 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-04-22 13:03:46,271 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-04-22 13:03:46,285 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-04-22 13:03:46,299 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-04-22 13:03:46,300 : INFO : training on 20305610 raw words (15602508 effective words) took 39.0s, 400369 effective words/s\n"
     ]
    }
   ],
   "source": [
    "#word vector training process\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "EMBEDDING_DIM=200\n",
    "# min_count: words with total frequency lower than this are ignored\n",
    "# size: the dimension of word vector\n",
    "# window: is the maximum distance \n",
    "#         between the current and predicted word \n",
    "#         within a sentence (i.e. the length of ngrams)\n",
    "# workers: # of parallel threads in training\n",
    "# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "wv_model = word2vec.Word2Vec(sentences, min_count=5, \\\n",
    "                             size=EMBEDDING_DIM, window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# get word vector for all words in the vocabulary\n",
    "\n",
    "MAX_NB_WORDS=6000\n",
    "\n",
    "# tokenizer.word_index provides the mapping \n",
    "# between a word and word index for all words\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train[\"reviews\"])\n",
    "NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "\n",
    "# \"+1\" is for padding symbol\n",
    "embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # if word_index is above the max number of words, ignore it\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    if word in wv_model.wv:\n",
    "        embedding_matrix[i]=wv_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "              \n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              EMBEDDING_DIM=200, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              PRETRAINED_WORD_VECTOR=embedding_matrix,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.01):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z=Concatenate(name='concate')(conv_blocks)\n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(192, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model\n",
    "\n",
    "#name best models\n",
    "BEST_MODEL_FILEPATH1=\"best_model1\"\n",
    "BEST_MODEL_FILEPATH2=\"best_model2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66053\n",
      "[('when', 11116), ('i', 104694), ('want', 4042), ('a', 111798), ('sandwich', 4041)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25c16210908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "# Define MAX_NB_WORDS  \n",
    "# Set MAX_NB_WORDS to include words that appear at least K times\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# total number of words\n",
    "total_nb_words=len(tokenizer.word_counts)\n",
    "print(total_nb_words)\n",
    "\n",
    "print(list(tokenizer.word_counts.items())[0:5])\n",
    "# put word and its counts into a data frame\n",
    "word_counts=pd.DataFrame(\\\n",
    "            list(tokenizer.word_counts.items()), \\\n",
    "            columns=['word','count'])\n",
    "word_counts.head(3)\n",
    "\n",
    "# get histogram of word counts\n",
    "# after reset index, \"index\" column \n",
    "# is the word frequency\n",
    "# \"count\" column gives how many words appear at \n",
    "# a specific frequency\n",
    "df=word_counts['count'].value_counts().reset_index()\n",
    "df.head(3)\n",
    "\n",
    "# convert absolute counts to precentage\n",
    "df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "# get cumulative percentage\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "df.head(5)\n",
    "\n",
    "# plot the chart\n",
    "# then we decided to set MAX_NB_WORDS=2000\n",
    "plt.bar(df[\"index\"].iloc[0:50], df[\"percent\"].iloc[0:50])\n",
    "plt.plot(df[\"index\"].iloc[0:50], df['cumsum'].iloc[0:50], c='green')\n",
    "\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25c18869a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define MAX_DOC_LEN \n",
    "# Set MAX_DOC_LEN to include complete sentences as many as possible\n",
    "\n",
    "# create a series based on the length of all sentences\n",
    "sequences_aspect = tokenizer.texts_to_sequences(aspect_l[\"Reviews\"])\n",
    "sen_len=pd.Series([len(item) for item in sequences_aspect])\n",
    "\n",
    "# create histogram of sentence length\n",
    "# the \"index\" is the sentence length\n",
    "# \"counts\" is the count of sentences at a length\n",
    "df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "df.columns=['index','counts']\n",
    "df.head(3)\n",
    "\n",
    "# sort by sentence length\n",
    "# get percentage and cumulative percentage\n",
    "\n",
    "df=df.sort_values(by='index')\n",
    "df['percent']=df['counts']/len(sen_len)\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "df.head(3)\n",
    "\n",
    "# From the plot, 90% sentences have length<30, and for both aspect and sentiment dataset the plot performence is alike\n",
    "# so we decided to set MAX_DOC_LEN=30 \n",
    "plt.plot(df[\"index\"], df['cumsum'], c='green')\n",
    "\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 382 samples, validate on 164 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.80030, saving model to best_model1\n",
      "2s - loss: 2.7937 - acc: 0.6780 - val_loss: 2.5318 - val_acc: 0.8003\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.80030 to 0.84299, saving model to best_model1\n",
      "0s - loss: 2.4860 - acc: 0.7683 - val_loss: 2.2529 - val_acc: 0.8430\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.84299 to 0.85823, saving model to best_model1\n",
      "0s - loss: 2.2645 - acc: 0.7866 - val_loss: 2.1108 - val_acc: 0.8582\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc improved from 0.85823 to 0.86128, saving model to best_model1\n",
      "0s - loss: 2.0940 - acc: 0.8266 - val_loss: 1.9906 - val_acc: 0.8613\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc improved from 0.86128 to 0.86585, saving model to best_model1\n",
      "0s - loss: 1.9684 - acc: 0.8541 - val_loss: 1.9143 - val_acc: 0.8659\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc improved from 0.86585 to 0.86738, saving model to best_model1\n",
      "0s - loss: 1.8545 - acc: 0.8704 - val_loss: 1.8055 - val_acc: 0.8674\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc improved from 0.86738 to 0.87957, saving model to best_model1\n",
      "0s - loss: 1.7448 - acc: 0.8907 - val_loss: 1.7090 - val_acc: 0.8796\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc did not improve\n",
      "0s - loss: 1.6540 - acc: 0.8907 - val_loss: 1.6273 - val_acc: 0.8704\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc improved from 0.87957 to 0.88415, saving model to best_model1\n",
      "0s - loss: 1.5459 - acc: 0.9084 - val_loss: 1.5539 - val_acc: 0.8841\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc improved from 0.88415 to 0.88720, saving model to best_model1\n",
      "0s - loss: 1.4667 - acc: 0.9195 - val_loss: 1.4779 - val_acc: 0.8872\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "0s - loss: 1.3558 - acc: 0.9352 - val_loss: 1.4073 - val_acc: 0.8811\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc did not improve\n",
      "0s - loss: 1.2706 - acc: 0.9490 - val_loss: 1.3380 - val_acc: 0.8872\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc improved from 0.88720 to 0.88872, saving model to best_model1\n",
      "0s - loss: 1.1929 - acc: 0.9601 - val_loss: 1.2807 - val_acc: 0.8887\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc improved from 0.88872 to 0.89177, saving model to best_model1\n",
      "0s - loss: 1.1220 - acc: 0.9601 - val_loss: 1.2181 - val_acc: 0.8918\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc improved from 0.89177 to 0.89482, saving model to best_model1\n",
      "0s - loss: 1.0585 - acc: 0.9660 - val_loss: 1.1608 - val_acc: 0.8948\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "0s - loss: 0.9845 - acc: 0.9732 - val_loss: 1.1091 - val_acc: 0.8933\n",
      "Epoch 17/100\n",
      "Epoch 00016: val_acc did not improve\n",
      "0s - loss: 0.9425 - acc: 0.9614 - val_loss: 1.0601 - val_acc: 0.8918\n",
      "Epoch 18/100\n",
      "Epoch 00017: val_acc improved from 0.89482 to 0.89787, saving model to best_model1\n",
      "0s - loss: 0.8717 - acc: 0.9791 - val_loss: 1.0242 - val_acc: 0.8979\n",
      "Epoch 19/100\n",
      "Epoch 00018: val_acc did not improve\n",
      "0s - loss: 0.8206 - acc: 0.9810 - val_loss: 0.9916 - val_acc: 0.8933\n",
      "Epoch 20/100\n",
      "Epoch 00019: val_acc did not improve\n",
      "0s - loss: 0.7806 - acc: 0.9777 - val_loss: 0.9372 - val_acc: 0.8918\n",
      "Epoch 21/100\n",
      "Epoch 00020: val_acc improved from 0.89787 to 0.90091, saving model to best_model1\n",
      "0s - loss: 0.7318 - acc: 0.9830 - val_loss: 0.8970 - val_acc: 0.9009\n",
      "Epoch 22/100\n",
      "Epoch 00021: val_acc improved from 0.90091 to 0.90091, saving model to best_model1\n",
      "0s - loss: 0.6879 - acc: 0.9797 - val_loss: 0.8659 - val_acc: 0.9009\n",
      "Epoch 23/100\n",
      "Epoch 00022: val_acc did not improve\n",
      "0s - loss: 0.6431 - acc: 0.9889 - val_loss: 0.8376 - val_acc: 0.8887\n",
      "Epoch 24/100\n",
      "Epoch 00023: val_acc did not improve\n",
      "0s - loss: 0.6029 - acc: 0.9869 - val_loss: 0.8069 - val_acc: 0.9009\n",
      "Epoch 25/100\n",
      "Epoch 00024: val_acc did not improve\n",
      "0s - loss: 0.5721 - acc: 0.9895 - val_loss: 0.7714 - val_acc: 0.8948\n",
      "Epoch 26/100\n",
      "Epoch 00025: val_acc did not improve\n",
      "0s - loss: 0.5398 - acc: 0.9876 - val_loss: 0.7436 - val_acc: 0.8994\n",
      "Epoch 27/100\n",
      "Epoch 00026: val_acc did not improve\n",
      "0s - loss: 0.5140 - acc: 0.9869 - val_loss: 0.7147 - val_acc: 0.8979\n",
      "Epoch 28/100\n",
      "Epoch 00027: val_acc did not improve\n",
      "0s - loss: 0.4720 - acc: 0.9921 - val_loss: 0.7041 - val_acc: 0.8994\n",
      "Epoch 29/100\n",
      "Epoch 00028: val_acc did not improve\n",
      "0s - loss: 0.4491 - acc: 0.9908 - val_loss: 0.6743 - val_acc: 0.9009\n",
      "Epoch 30/100\n",
      "Epoch 00029: val_acc did not improve\n",
      "0s - loss: 0.4234 - acc: 0.9928 - val_loss: 0.6546 - val_acc: 0.8963\n",
      "Epoch 31/100\n",
      "Epoch 00030: val_acc did not improve\n",
      "0s - loss: 0.3985 - acc: 0.9928 - val_loss: 0.6277 - val_acc: 0.8979\n",
      "Epoch 32/100\n",
      "Epoch 00031: val_acc did not improve\n",
      "0s - loss: 0.3789 - acc: 0.9902 - val_loss: 0.5975 - val_acc: 0.8994\n",
      "Epoch 33/100\n",
      "Epoch 00032: val_acc did not improve\n",
      "0s - loss: 0.3491 - acc: 0.9987 - val_loss: 0.5855 - val_acc: 0.8948\n",
      "Epoch 34/100\n",
      "Epoch 00033: val_acc improved from 0.90091 to 0.90549, saving model to best_model1\n",
      "0s - loss: 0.3343 - acc: 0.9915 - val_loss: 0.5730 - val_acc: 0.9055\n",
      "Epoch 35/100\n",
      "Epoch 00034: val_acc did not improve\n",
      "0s - loss: 0.3144 - acc: 0.9948 - val_loss: 0.5511 - val_acc: 0.8933\n",
      "Epoch 36/100\n",
      "Epoch 00035: val_acc did not improve\n",
      "0s - loss: 0.2959 - acc: 0.9928 - val_loss: 0.5379 - val_acc: 0.8994\n",
      "Epoch 37/100\n",
      "Epoch 00036: val_acc improved from 0.90549 to 0.90701, saving model to best_model1\n",
      "0s - loss: 0.2801 - acc: 0.9961 - val_loss: 0.5304 - val_acc: 0.9070\n",
      "Epoch 38/100\n",
      "Epoch 00037: val_acc did not improve\n",
      "0s - loss: 0.2641 - acc: 0.9954 - val_loss: 0.5083 - val_acc: 0.8933\n",
      "Epoch 39/100\n",
      "Epoch 00038: val_acc did not improve\n",
      "0s - loss: 0.2487 - acc: 0.9980 - val_loss: 0.4981 - val_acc: 0.9024\n",
      "Epoch 40/100\n",
      "Epoch 00039: val_acc did not improve\n",
      "0s - loss: 0.2415 - acc: 0.9948 - val_loss: 0.4894 - val_acc: 0.9055\n",
      "Epoch 41/100\n",
      "Epoch 00040: val_acc did not improve\n",
      "0s - loss: 0.2255 - acc: 0.9961 - val_loss: 0.4583 - val_acc: 0.9009\n",
      "Epoch 42/100\n",
      "Epoch 00041: val_acc did not improve\n",
      "0s - loss: 0.2144 - acc: 0.9948 - val_loss: 0.4626 - val_acc: 0.8933\n",
      "Epoch 00041: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fit aspect model using pretrained word vectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set the number of output units\n",
    "NUM_OUTPUT_UNITS=len(mlb1.classes_)\n",
    "\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# We used 400 labeled aspect reviews\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "MAX_DOC_LEN=30\n",
    "#sequences_aspect = tokenizer.texts_to_sequences(aspect_l[\"Reviews\"])\n",
    "padded_sequences_aspect = pad_sequences(sequences_aspect, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "#\n",
    "\n",
    "#\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences_aspect[0:546], Y1[0:546], \\\n",
    "                test_size=0.3, random_state=0)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, NUM_OUTPUT_UNITS, \\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH1, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 96.84%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of aspect CNN model\n",
    "scores_1 = model.evaluate(padded_sequences_aspect, Y1, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_1[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 287 samples, validate on 123 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.62195, saving model to best_model2\n",
      "0s - loss: 2.8678 - acc: 0.5418 - val_loss: 2.5441 - val_acc: 0.6220\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc did not improve\n",
      "0s - loss: 2.5571 - acc: 0.6481 - val_loss: 2.5231 - val_acc: 0.5894\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.62195 to 0.65041, saving model to best_model2\n",
      "0s - loss: 2.4674 - acc: 0.6794 - val_loss: 2.3825 - val_acc: 0.6504\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc did not improve\n",
      "0s - loss: 2.3259 - acc: 0.6934 - val_loss: 2.3155 - val_acc: 0.6301\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc improved from 0.65041 to 0.67886, saving model to best_model2\n",
      "0s - loss: 2.1631 - acc: 0.7822 - val_loss: 2.2354 - val_acc: 0.6789\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc did not improve\n",
      "0s - loss: 2.0263 - acc: 0.8240 - val_loss: 2.1632 - val_acc: 0.6626\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc improved from 0.67886 to 0.69512, saving model to best_model2\n",
      "0s - loss: 1.9247 - acc: 0.8432 - val_loss: 2.0913 - val_acc: 0.6951\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc did not improve\n",
      "0s - loss: 1.8037 - acc: 0.8798 - val_loss: 2.0353 - val_acc: 0.6911\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc improved from 0.69512 to 0.69919, saving model to best_model2\n",
      "0s - loss: 1.7234 - acc: 0.8885 - val_loss: 1.9951 - val_acc: 0.6992\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc improved from 0.69919 to 0.71545, saving model to best_model2\n",
      "0s - loss: 1.6097 - acc: 0.9216 - val_loss: 1.9470 - val_acc: 0.7154\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "0s - loss: 1.5456 - acc: 0.9251 - val_loss: 1.8978 - val_acc: 0.6992\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc did not improve\n",
      "0s - loss: 1.4774 - acc: 0.9321 - val_loss: 1.8559 - val_acc: 0.7114\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc improved from 0.71545 to 0.73984, saving model to best_model2\n",
      "0s - loss: 1.3711 - acc: 0.9460 - val_loss: 1.8540 - val_acc: 0.7398\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc did not improve\n",
      "0s - loss: 1.3414 - acc: 0.9477 - val_loss: 1.7701 - val_acc: 0.7154\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc did not improve\n",
      "0s - loss: 1.2648 - acc: 0.9512 - val_loss: 1.7221 - val_acc: 0.7154\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "0s - loss: 1.1793 - acc: 0.9774 - val_loss: 1.7603 - val_acc: 0.7358\n",
      "Epoch 00015: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fit sentiment model using pretrained word vectors\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "from sklearn.model_selection import train_test_split\n",
    "NUM_OUTPUT_UNITS_1=len(mlb2.classes_)\n",
    "\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# We use our 450 labeled sentiment reviews\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "MAX_DOC_LEN=30\n",
    "sequences_sentiment = tokenizer.texts_to_sequences(sentiment_l[\"Reviews\"])\n",
    "padded_sequences_sentiment = pad_sequences(sequences_sentiment, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "#sequences_2 = tokenizer.texts_to_sequences(predict[1])\n",
    "#padded_sequences_2 = pad_sequences(sequences_2, \\\n",
    "                                 #maxlen=MAX_DOC_LEN, \\\n",
    "                                 #padding='post', \\\n",
    "                                 #truncating='post')\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(\\\n",
    "                padded_sequences_sentiment[0:410], Y2[0:410], \\\n",
    "                test_size=0.3, random_state=0)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model_1=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, NUM_OUTPUT_UNITS_1, \\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping_1=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint_1 = ModelCheckpoint(BEST_MODEL_FILEPATH2, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training_1=model_1.fit(X_train_1, Y_train_1, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping_1, checkpoint_1],\\\n",
    "          validation_data=[X_test_1, Y_test_1], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 91.47%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of sentiment CNN model\n",
    "scores_2 = model_1.evaluate(padded_sequences_sentiment, Y2, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_1.metrics_names[1], scores_2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad reviews for prediction\n",
    "sequences_1 = tokenizer.texts_to_sequences(predict[\"reviews\"])\n",
    "padded_sequences_1 = pad_sequences(sequences_1, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model configuration of aspect model\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 30, 200)       1200200     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                  (None, 29, 64)        25664       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                  (None, 28, 64)        38464       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_4 (Conv1D)                  (None, 27, 64)        51264       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_2 (MaxPooling1D)             (None, 1, 64)         0           conv_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_3 (MaxPooling1D)             (None, 1, 64)         0           conv_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_4 (MaxPooling1D)             (None, 1, 64)         0           conv_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flat_2 (Flatten)                 (None, 64)            0           max_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_3 (Flatten)                 (None, 64)            0           max_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_4 (Flatten)                 (None, 64)            0           max_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concate (Concatenate)            (None, 192)           0           flat_2[0][0]                     \n",
      "                                                                   flat_3[0][0]                     \n",
      "                                                                   flat_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 192)           0           concate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 192)           37056       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 4)             772         dense[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,353,420\n",
      "Trainable params: 153,220\n",
      "Non-trainable params: 1,200,200\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "model configuration of sentiment model\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 30, 200)       1200200     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                  (None, 29, 64)        25664       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                  (None, 28, 64)        38464       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_4 (Conv1D)                  (None, 27, 64)        51264       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_2 (MaxPooling1D)             (None, 1, 64)         0           conv_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_3 (MaxPooling1D)             (None, 1, 64)         0           conv_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_4 (MaxPooling1D)             (None, 1, 64)         0           conv_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flat_2 (Flatten)                 (None, 64)            0           max_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_3 (Flatten)                 (None, 64)            0           max_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_4 (Flatten)                 (None, 64)            0           max_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concate (Concatenate)            (None, 192)           0           flat_2[0][0]                     \n",
      "                                                                   flat_3[0][0]                     \n",
      "                                                                   flat_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 192)           0           concate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 192)           37056       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 2)             386         dense[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,353,034\n",
      "Trainable params: 152,834\n",
      "Non-trainable params: 1,200,200\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check model configuration\n",
    "print(\"model configuration of aspect model\")\n",
    "print(model.summary())\n",
    "print(\"\\n\")\n",
    "print(\"model configuration of sentiment model\")\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   ambience       0.72      0.62      0.67        21\n",
      "       food       0.83      0.89      0.86        95\n",
      "      price       0.85      0.61      0.71        18\n",
      "    service       0.89      0.55      0.68        31\n",
      "\n",
      "avg / total       0.83      0.76      0.79       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation of aspect CNN\n",
    "# Let's use samples[0:400]\n",
    "# as an evaluation set\n",
    "from sklearn.metrics import classification_report\n",
    "pred_1=model.predict(X_test)\n",
    "\n",
    "Y_pred_1=np.matrix(pred_1)\n",
    "Y_pred_1=np.where(Y_pred_1>0.3,1,0)\n",
    "#Y1=np.array(Y1)\n",
    "\n",
    "#Y_pred_1[0:10]\n",
    "#Y1[100:110]\n",
    "\n",
    "#y = map(lambda x: int(x), y)\n",
    "#answer = map(lambda x: int(x), answer)\n",
    "print(classification_report(Y_test, Y_pred_1, target_names=mlb1.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negative       0.76      0.50      0.60        50\n",
      "   Positive       0.73      0.89      0.80        73\n",
      "\n",
      "avg / total       0.74      0.73      0.72       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation of sentiment CNN\n",
    "# Let's use samples[0:400]\n",
    "# as an evaluation set\n",
    "from sklearn.metrics import classification_report\n",
    "pred_2=model_1.predict(X_test_1)\n",
    "\n",
    "Y_pred_2=np.matrix(pred_2)\n",
    "Y_pred_2=np.where(Y_pred_2>0.5,1,0)\n",
    "#Y=np.array(Y)\n",
    "\n",
    "#Y_pred_2[0:10]\n",
    "#Y2[100:110]\n",
    "\n",
    "#y = map(lambda x: int(x), y)\n",
    "#answer = map(lambda x: int(x), answer)\n",
    "print(classification_report(Y_test_1, Y_pred_2, target_names=mlb2.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (2, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (14, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (18, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (33, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (38, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (49, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (55, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (56, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (61, array([0, 0, 1, 0]), array([0, 0, 0, 0])), (64, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (67, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (80, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (88, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (111, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (112, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (140, array([1, 0, 1, 0]), array([0, 0, 0, 0])), (149, array([1, 0, 0, 0]), array([0, 0, 0, 0])), (155, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (159, array([1, 0, 0, 0]), array([0, 0, 0, 0]))]\n"
     ]
    }
   ],
   "source": [
    "errors_a=[]\n",
    "for idx, row in enumerate(Y_test):\n",
    "    if set(row) != set(Y_pred_1[idx]):\n",
    "        errors_a.append((idx, row, Y_pred_1[idx]))\n",
    "print(errors_a)\n",
    "\n",
    "#[i for i, j in zip(a, b) if i == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(73, array([1, 0]), array([0, 0]))]\n"
     ]
    }
   ],
   "source": [
    "errors=[]\n",
    "for idx, row in enumerate(Y_test_1):\n",
    "    if set(row) != set(Y_pred_2[idx]):\n",
    "        errors.append((idx, row, Y_pred_2[idx]))\n",
    "print(errors)\n",
    "\n",
    "#[i for i, j in zip(a, b) if i == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ambience' 'food' 'price' 'service']\n",
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [1 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 1 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [1 1 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 1 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#Predict the unlabelled dataset with aspects, and save the result into a csv file\n",
    "pred_11=model.predict(padded_sequences_1[0:])\n",
    "\n",
    "#we chose a threshold of 0.3, because at 30% probability, every review can have a label, and some aspects that \n",
    "#could be mentioned but not mentioned a lot in the review could be reflected\n",
    "Y_pred_11=np.matrix(pred_11)\n",
    "Y_pred_11=np.where(Y_pred_11>0.3,1,0)\n",
    "\n",
    "#name of each column\n",
    "print(mlb1.classes_)\n",
    "print(Y_pred_11[0:200])\n",
    "\n",
    "result_1 = pd.DataFrame(Y_pred_11)\n",
    "result_1.to_csv('output1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Positive']\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#Predict the unlabelled dataset with sentiments, and save the result into a csv file\n",
    "pred_12=model_1.predict(padded_sequences_1[0:])\n",
    "Y_pred_12=np.matrix(pred_12)\n",
    "#print(Y_pred_12)\n",
    "\n",
    "#Because each review can only have a sentiment as a whole, we chose sentiment with the highest probability for each reveiw \n",
    "#as its representitive sentiment. And the highest probability is converted to 1, others 0.\n",
    "print(mlb2.classes_)\n",
    "print((Y_pred_12 == Y_pred_12.max(axis=1)).astype(float))\n",
    "\n",
    "result_2 = pd.DataFrame((Y_pred_12 == Y_pred_12.max(axis=1)).astype(float))\n",
    "result_2.to_csv('output2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
