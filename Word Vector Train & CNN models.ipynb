{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lacey\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from numpy.random import shuffle\n",
    "import pandas as pd\n",
    "import nltk,string\n",
    "from gensim import corpora\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When I want a sandwich, this is where I want i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't think I've ever had a good experience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretty standard Chinese food.  Never had a bad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This little diner is a staple in Bloomfield's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We've been to this location a few times but I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  When I want a sandwich, this is where I want i...\n",
       "1  I don't think I've ever had a good experience ...\n",
       "2  Pretty standard Chinese food.  Never had a bad...\n",
       "3  This little diner is a staple in Bloomfield's ...\n",
       "4  We've been to this location a few times but I'..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load reviews for wordvector training\n",
    "train=pd.read_csv(\"wordvector_train.csv\",header=0,delimiter=\"\\t\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>duHFBe87uNSXImQmvBh87Q</td>\n",
       "      <td>When I want a sandwich, this is where I want i...</td>\n",
       "      <td>['Sandwiches', 'Restaurants']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SDMRxmcKPNt1AHPBKqO64Q</td>\n",
       "      <td>I don't think I've ever had a good experience ...</td>\n",
       "      <td>['Burgers', 'Bars', 'Restaurants', 'Sports Bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iFEiMJoEqyB9O8OUNSdLzA</td>\n",
       "      <td>Pretty standard Chinese food.  Never had a bad...</td>\n",
       "      <td>['Chinese', 'Restaurants']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HmI9nhgOkrXlUr6KZGZZew</td>\n",
       "      <td>This little diner is a staple in Bloomfield's ...</td>\n",
       "      <td>['Sandwiches', 'Restaurants', 'Italian', 'Dine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qnpvw-uQyRn9nlClWFK9aA</td>\n",
       "      <td>We've been to this location a few times but I'...</td>\n",
       "      <td>['Chicken Wings', 'Restaurants']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business id                                            reviews  \\\n",
       "0  duHFBe87uNSXImQmvBh87Q  When I want a sandwich, this is where I want i...   \n",
       "1  SDMRxmcKPNt1AHPBKqO64Q  I don't think I've ever had a good experience ...   \n",
       "2  iFEiMJoEqyB9O8OUNSdLzA  Pretty standard Chinese food.  Never had a bad...   \n",
       "3  HmI9nhgOkrXlUr6KZGZZew  This little diner is a staple in Bloomfield's ...   \n",
       "4  qnpvw-uQyRn9nlClWFK9aA  We've been to this location a few times but I'...   \n",
       "\n",
       "                                          categories  \n",
       "0                      ['Sandwiches', 'Restaurants']  \n",
       "1  ['Burgers', 'Bars', 'Restaurants', 'Sports Bar...  \n",
       "2                         ['Chinese', 'Restaurants']  \n",
       "3  ['Sandwiches', 'Restaurants', 'Italian', 'Dine...  \n",
       "4                   ['Chicken Wings', 'Restaurants']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load parsed dataset for aspect and sentiment prediction\n",
    "predict=pd.read_csv(\"reviews+categories.csv\",header=0,delimiter=\",\")\n",
    "predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The food was at best mediocre</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I recommend the lemon chicken, the honey garl...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This place is a hole in the wall.</td>\n",
       "      <td>ambience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The service is amazing</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>but a great cheap lunch menu.</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews   Aspects\n",
       "0                      The food was at best mediocre      food\n",
       "1   I recommend the lemon chicken, the honey garl...      food\n",
       "2                  This place is a hole in the wall.  ambience\n",
       "3                            The service is amazing    service\n",
       "4                      but a great cheap lunch menu.     price"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load labelled aspect reviews for training \n",
    "aspect_l=pd.read_csv(\"labelled1.csv\", header=0, delimiter=\",\")\n",
    "#shuffle data\n",
    "aspect_l = aspect_l.sample(frac=1).reset_index(drop=True)\n",
    "aspect_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If you're used to the crunchy, shrimp tempura...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bathrooms, if you are going to have a big dri...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as for the soup, I had the broccoli/cheese wh...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You'll enjoy scooping it up with their homema...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very big, with a great location next to the b...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews sentiment\n",
       "0   If you're used to the crunchy, shrimp tempura...  Positive\n",
       "1   Bathrooms, if you are going to have a big dri...  Negative\n",
       "2   as for the soup, I had the broccoli/cheese wh...  Positive\n",
       "3   You'll enjoy scooping it up with their homema...  Positive\n",
       "4   Very big, with a great location next to the b...  Positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load labelled aspect reviews for training \n",
    "sentiment_l=pd.read_csv(\"labelled3.csv\", header=0, delimiter=\",\")\n",
    "#shuffle data\n",
    "sentiment_l = sentiment_l.sample(frac=1).reset_index(drop=True)\n",
    "sentiment_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample label of labelled aspects\n",
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]]\n",
      "Sample label of labelled sentiments\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "#convert labels into the form of \"list of list\"\n",
    "labels1=[str(item[\"Aspects\"]).split(\",\") for index, item in aspect_l.iterrows()]\n",
    "labels2=[str(item[\"sentiment\"]).split(\",\") for index, item in sentiment_l.iterrows()]\n",
    "#create indicator matrix for labels\n",
    "mlb1=MultiLabelBinarizer()\n",
    "mlb2=MultiLabelBinarizer()\n",
    "Y1=mlb1.fit_transform(labels1)\n",
    "Y2=mlb2.fit_transform(labels2)\n",
    "print(\"Sample label of labelled aspects\")\n",
    "print(Y1[0:5])\n",
    "print(\"Sample label of labelled sentiments\")\n",
    "print(Y2[0:5])\n",
    "#mlb1.classes_\n",
    "#mlb2.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['When', 'want', 'sandwich', 'this', 'is', 'where', 'want', 'it', 'from', 'Soft', 'flavorful', 'bread', 'you', 'are', 'already', 'ahead', 'of', 'the', 'game', 'because', 'Subway', 'and', 'Jimmy', 'John', 'have', 'hard', 'and/or', 'bland', 'bread', 'Fresh', 'sliced', 'meats', 'cheeses', 'and', 'veggies', 'sorry', 'but', 'JJ', 'have', 'slimy', 'meat', 'and', 'that', 'HUGE', 'turn-off', 'and', 'when', 'you', 'take', 'bite', 'of', 'your', 'sandwich', 'and', 'all', 'of', 'the', 'mayo/mustard/vinegar/oregano', 'have', 'magically', 'melded', 'together', 'you', 'll', 'be', 'believer', 'too', 'Prices', 'are', 'slightly', 'higher', 'than', 'Subway', 'pretty', 'similar', 'to', 'Jimmy', 'John', 'but', 'the', 'quality', 'leaves', 'them', 'in', 'the', 'dust', 'My', 'only', 'suggestion', 'would', 'be', 'for', 'them', 'to', 'start', 'delivery', 'service', 'or', 'subscribe', 'to', 'Grubhub.com', 'or', 'something', 'like', 'that', 'so', 'could', 'order', 'it', 'for', 'our', 'lunch', 'meetings', 'Oh', 'and', 'the', 'service', 'is', 'always', 'done', 'with', 'smile', 'Great', 'job', 'guys'], ['do', \"n't\", 'think', 've', 'ever', 'had', 'good', 'experience', 'here', 'They', 'either', 'get', 'my', 'order', 'wrong', 'and/or', 'the', 'food', 'is', 'disgusting']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize reviews in wordvector_train.csv and prepare for word vector training\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd\n",
    "sentences=[ [token.strip(string.punctuation).strip() \\\n",
    "             for token in nltk.word_tokenize(doc) \\\n",
    "                 if token not in string.punctuation and \\\n",
    "                 len(token.strip(string.punctuation).strip())>=2]\\\n",
    "             for doc in train['reviews']]\n",
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-08 17:47:51,942 : INFO : collecting all words and their counts\n",
      "2017-12-08 17:47:51,944 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-08 17:47:52,225 : INFO : PROGRESS: at sentence #10000, processed 1003962 words, keeping 41079 word types\n",
      "2017-12-08 17:47:52,473 : INFO : PROGRESS: at sentence #20000, processed 1996168 words, keeping 60409 word types\n",
      "2017-12-08 17:47:52,730 : INFO : PROGRESS: at sentence #30000, processed 2980257 words, keeping 75720 word types\n",
      "2017-12-08 17:47:53,032 : INFO : PROGRESS: at sentence #40000, processed 3989289 words, keeping 89728 word types\n",
      "2017-12-08 17:47:53,048 : INFO : collected 90545 word types from a corpus of 4061122 raw words and 41090 sentences\n",
      "2017-12-08 17:47:53,050 : INFO : Loading a fresh vocabulary\n",
      "2017-12-08 17:47:53,162 : INFO : min_count=5 retains 21421 unique words (23% of original 90545, drops 69124)\n",
      "2017-12-08 17:47:53,165 : INFO : min_count=5 leaves 3958990 word corpus (97% of original 4061122, drops 102132)\n",
      "2017-12-08 17:47:53,254 : INFO : deleting the raw counts dictionary of 90545 items\n",
      "2017-12-08 17:47:53,259 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2017-12-08 17:47:53,261 : INFO : downsampling leaves estimated 3120690 word corpus (78.8% of prior 3958990)\n",
      "2017-12-08 17:47:53,262 : INFO : estimated required memory for 21421 words and 200 dimensions: 44984100 bytes\n",
      "2017-12-08 17:47:53,381 : INFO : resetting layer weights\n",
      "2017-12-08 17:47:53,757 : INFO : training model with 4 workers on 21421 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-12-08 17:47:54,770 : INFO : PROGRESS: at 4.00% examples, 626689 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-08 17:47:55,776 : INFO : PROGRESS: at 9.06% examples, 709905 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-08 17:47:56,781 : INFO : PROGRESS: at 13.78% examples, 715970 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-08 17:47:57,782 : INFO : PROGRESS: at 18.72% examples, 732949 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-08 17:47:58,794 : INFO : PROGRESS: at 24.72% examples, 768309 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:47:59,802 : INFO : PROGRESS: at 30.05% examples, 778797 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:00,813 : INFO : PROGRESS: at 35.05% examples, 777411 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:01,824 : INFO : PROGRESS: at 40.10% examples, 776073 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-08 17:48:02,826 : INFO : PROGRESS: at 44.62% examples, 769119 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:03,836 : INFO : PROGRESS: at 49.96% examples, 775119 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:04,842 : INFO : PROGRESS: at 55.44% examples, 781835 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-08 17:48:05,845 : INFO : PROGRESS: at 60.73% examples, 784890 words/s, in_qsize 6, out_qsize 1\n",
      "2017-12-08 17:48:06,850 : INFO : PROGRESS: at 66.50% examples, 793810 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:07,850 : INFO : PROGRESS: at 72.25% examples, 801139 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:08,853 : INFO : PROGRESS: at 77.52% examples, 802997 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-08 17:48:09,870 : INFO : PROGRESS: at 82.91% examples, 803721 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:10,883 : INFO : PROGRESS: at 88.64% examples, 808621 words/s, in_qsize 8, out_qsize 0\n",
      "2017-12-08 17:48:11,886 : INFO : PROGRESS: at 94.32% examples, 812661 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:12,894 : INFO : PROGRESS: at 99.49% examples, 812734 words/s, in_qsize 7, out_qsize 0\n",
      "2017-12-08 17:48:12,930 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-08 17:48:12,947 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-08 17:48:12,950 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-08 17:48:12,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-08 17:48:12,955 : INFO : training on 20305610 raw words (15603246 effective words) took 19.2s, 812926 effective words/s\n"
     ]
    }
   ],
   "source": [
    "#word vector training process\n",
    "# print out tracking information\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "EMBEDDING_DIM=200\n",
    "# min_count: words with total frequency lower than this are ignored\n",
    "# size: the dimension of word vector\n",
    "# window: is the maximum distance \n",
    "#         between the current and predicted word \n",
    "#         within a sentence (i.e. the length of ngrams)\n",
    "# workers: # of parallel threads in training\n",
    "# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n",
    "wv_model = word2vec.Word2Vec(sentences, min_count=5, \\\n",
    "                             size=EMBEDDING_DIM, window=5, workers=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# get word vector for all words in the vocabulary\n",
    "\n",
    "MAX_NB_WORDS=6000\n",
    "\n",
    "# tokenizer.word_index provides the mapping \n",
    "# between a word and word index for all words\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train[\"reviews\"])\n",
    "NUM_WORDS = min(MAX_NB_WORDS, len(tokenizer.word_index))\n",
    "\n",
    "# \"+1\" is for padding symbol\n",
    "embedding_matrix = np.zeros((NUM_WORDS+1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    # if word_index is above the max number of words, ignore it\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    if word in wv_model.wv:\n",
    "        embedding_matrix[i]=wv_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "              \n",
    "def cnn_model(FILTER_SIZES, \\\n",
    "              # filter sizes as a list\n",
    "              MAX_NB_WORDS, \\\n",
    "              # total number of words\n",
    "              MAX_DOC_LEN, \\\n",
    "              # max words in a doc\n",
    "              NUM_OUTPUT_UNITS=1, \\\n",
    "              # number of output units\n",
    "              EMBEDDING_DIM=200, \\\n",
    "              # word vector dimension\n",
    "              NUM_FILTERS=64, \\\n",
    "              # number of filters for all size\n",
    "              DROP_OUT=0.5, \\\n",
    "              # dropout rate\n",
    "              PRETRAINED_WORD_VECTOR=embedding_matrix,\\\n",
    "              # Whether to use pretrained word vectors\n",
    "              LAM=0.01):            \n",
    "              # regularization coefficient\n",
    "    \n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), \\\n",
    "                       dtype='int32', name='main_input')\n",
    "    \n",
    "    if PRETRAINED_WORD_VECTOR is not None:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        weights=[PRETRAINED_WORD_VECTOR],\\\n",
    "                        trainable=False,\\\n",
    "                        name='embedding')(main_input)\n",
    "    else:\n",
    "        embed_1 = Embedding(input_dim=MAX_NB_WORDS+1, \\\n",
    "                        output_dim=EMBEDDING_DIM, \\\n",
    "                        input_length=MAX_DOC_LEN, \\\n",
    "                        name='embedding')(main_input)\n",
    "    # add convolution-pooling-flat block\n",
    "    conv_blocks = []\n",
    "    for f in FILTER_SIZES:\n",
    "        conv = Conv1D(filters=NUM_FILTERS, kernel_size=f, \\\n",
    "                      activation='relu', name='conv_'+str(f))(embed_1)\n",
    "        conv = MaxPooling1D(MAX_DOC_LEN-f+1, name='max_'+str(f))(conv)\n",
    "        conv = Flatten(name='flat_'+str(f))(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    z=Concatenate(name='concate')(conv_blocks)\n",
    "    drop=Dropout(rate=DROP_OUT, name='dropout')(z)\n",
    "\n",
    "    dense = Dense(192, activation='relu',\\\n",
    "                    kernel_regularizer=l2(LAM),name='dense')(drop)\n",
    "    preds = Dense(NUM_OUTPUT_UNITS, activation='sigmoid', name='output')(dense)\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "    \n",
    "    model.compile(loss=\"binary_crossentropy\", \\\n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    return model\n",
    "\n",
    "#name best models\n",
    "BEST_MODEL_FILEPATH1=\"best_model1\"\n",
    "BEST_MODEL_FILEPATH2=\"best_model2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66053\n",
      "[('when', 11116), ('i', 104694), ('want', 4042), ('a', 111798), ('sandwich', 4041)]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1881c1552b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "# Define MAX_NB_WORDS  \n",
    "# Set MAX_NB_WORDS to include words that appear at least K times\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# total number of words\n",
    "total_nb_words=len(tokenizer.word_counts)\n",
    "print(total_nb_words)\n",
    "\n",
    "print(list(tokenizer.word_counts.items())[0:5])\n",
    "# put word and its counts into a data frame\n",
    "word_counts=pd.DataFrame(\\\n",
    "            list(tokenizer.word_counts.items()), \\\n",
    "            columns=['word','count'])\n",
    "word_counts.head(3)\n",
    "\n",
    "# get histogram of word counts\n",
    "# after reset index, \"index\" column \n",
    "# is the word frequency\n",
    "# \"count\" column gives how many words appear at \n",
    "# a specific frequency\n",
    "df=word_counts['count'].value_counts().reset_index()\n",
    "df.head(3)\n",
    "\n",
    "# convert absolute counts to precentage\n",
    "df['percent']=df['count']/len(tokenizer.word_counts)\n",
    "# get cumulative percentage\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "df.head(5)\n",
    "\n",
    "# plot the chart\n",
    "# then we decided to set MAX_NB_WORDS=2000\n",
    "plt.bar(df[\"index\"].iloc[0:50], df[\"percent\"].iloc[0:50])\n",
    "plt.plot(df[\"index\"].iloc[0:50], df['cumsum'].iloc[0:50], c='green')\n",
    "\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequences_aspect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2c4132f0dab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# create a series based on the length of all sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msen_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequences_aspect\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# create histogram of sentence length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sequences_aspect' is not defined"
     ]
    }
   ],
   "source": [
    "# Define MAX_DOC_LEN \n",
    "# Set MAX_DOC_LEN to include complete sentences as many as possible\n",
    "\n",
    "# create a series based on the length of all sentences\n",
    "sen_len=pd.Series([len(item) for item in sequences_aspect])\n",
    "\n",
    "# create histogram of sentence length\n",
    "# the \"index\" is the sentence length\n",
    "# \"counts\" is the count of sentences at a length\n",
    "df=sen_len.value_counts().reset_index().sort_values(by='index')\n",
    "df.columns=['index','counts']\n",
    "df.head(3)\n",
    "\n",
    "# sort by sentence length\n",
    "# get percentage and cumulative percentage\n",
    "\n",
    "df=df.sort_values(by='index')\n",
    "df['percent']=df['counts']/len(sen_len)\n",
    "df['cumsum']=df['percent'].cumsum()\n",
    "df.head(3)\n",
    "\n",
    "# From the plot, 90% sentences have length<30, and for both aspect and sentiment dataset the plot performence is alike\n",
    "# so we decided to set MAX_DOC_LEN=30 \n",
    "plt.plot(df[\"index\"], df['cumsum'], c='green')\n",
    "\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 382 samples, validate on 164 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.80488, saving model to best_model1\n",
      "0s - loss: 2.7598 - acc: 0.6806 - val_loss: 2.4066 - val_acc: 0.8049\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc improved from 0.80488 to 0.84146, saving model to best_model1\n",
      "0s - loss: 2.4672 - acc: 0.7624 - val_loss: 2.2162 - val_acc: 0.8415\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.84146 to 0.84756, saving model to best_model1\n",
      "0s - loss: 2.2418 - acc: 0.8004 - val_loss: 2.0651 - val_acc: 0.8476\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc improved from 0.84756 to 0.87043, saving model to best_model1\n",
      "0s - loss: 2.0846 - acc: 0.8194 - val_loss: 1.9621 - val_acc: 0.8704\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc did not improve\n",
      "0s - loss: 1.9516 - acc: 0.8390 - val_loss: 1.8563 - val_acc: 0.8659\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc improved from 0.87043 to 0.89177, saving model to best_model1\n",
      "0s - loss: 1.8051 - acc: 0.8776 - val_loss: 1.7524 - val_acc: 0.8918\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc did not improve\n",
      "0s - loss: 1.6941 - acc: 0.8927 - val_loss: 1.6526 - val_acc: 0.8902\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc improved from 0.89177 to 0.89482, saving model to best_model1\n",
      "0s - loss: 1.5829 - acc: 0.9110 - val_loss: 1.5629 - val_acc: 0.8948\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc improved from 0.89482 to 0.90244, saving model to best_model1\n",
      "0s - loss: 1.4881 - acc: 0.9116 - val_loss: 1.4777 - val_acc: 0.9024\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc improved from 0.90244 to 0.91311, saving model to best_model1\n",
      "0s - loss: 1.3846 - acc: 0.9306 - val_loss: 1.3996 - val_acc: 0.9131\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc improved from 0.91311 to 0.91616, saving model to best_model1\n",
      "0s - loss: 1.2868 - acc: 0.9424 - val_loss: 1.3249 - val_acc: 0.9162\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc did not improve\n",
      "0s - loss: 1.2183 - acc: 0.9418 - val_loss: 1.2564 - val_acc: 0.9146\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc did not improve\n",
      "0s - loss: 1.1274 - acc: 0.9581 - val_loss: 1.1900 - val_acc: 0.9101\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc did not improve\n",
      "0s - loss: 1.0552 - acc: 0.9653 - val_loss: 1.1285 - val_acc: 0.9116\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc did not improve\n",
      "0s - loss: 0.9900 - acc: 0.9705 - val_loss: 1.0733 - val_acc: 0.9146\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "0s - loss: 0.9268 - acc: 0.9660 - val_loss: 1.0226 - val_acc: 0.9101\n",
      "Epoch 17/100\n",
      "Epoch 00016: val_acc did not improve\n",
      "0s - loss: 0.8769 - acc: 0.9679 - val_loss: 0.9737 - val_acc: 0.9116\n",
      "Epoch 18/100\n",
      "Epoch 00017: val_acc did not improve\n",
      "0s - loss: 0.8113 - acc: 0.9771 - val_loss: 0.9307 - val_acc: 0.9116\n",
      "Epoch 19/100\n",
      "Epoch 00018: val_acc did not improve\n",
      "0s - loss: 0.7573 - acc: 0.9797 - val_loss: 0.8840 - val_acc: 0.9146\n",
      "Epoch 20/100\n",
      "Epoch 00019: val_acc did not improve\n",
      "0s - loss: 0.7076 - acc: 0.9830 - val_loss: 0.8447 - val_acc: 0.9116\n",
      "Epoch 21/100\n",
      "Epoch 00020: val_acc improved from 0.91616 to 0.91616, saving model to best_model1\n",
      "0s - loss: 0.6638 - acc: 0.9817 - val_loss: 0.8090 - val_acc: 0.9162\n",
      "Epoch 22/100\n",
      "Epoch 00021: val_acc did not improve\n",
      "0s - loss: 0.6199 - acc: 0.9869 - val_loss: 0.7767 - val_acc: 0.9101\n",
      "Epoch 23/100\n",
      "Epoch 00022: val_acc did not improve\n",
      "0s - loss: 0.5888 - acc: 0.9856 - val_loss: 0.7367 - val_acc: 0.9116\n",
      "Epoch 24/100\n",
      "Epoch 00023: val_acc did not improve\n",
      "0s - loss: 0.5417 - acc: 0.9869 - val_loss: 0.7189 - val_acc: 0.9085\n",
      "Epoch 25/100\n",
      "Epoch 00024: val_acc did not improve\n",
      "0s - loss: 0.5070 - acc: 0.9902 - val_loss: 0.6785 - val_acc: 0.9101\n",
      "Epoch 26/100\n",
      "Epoch 00025: val_acc did not improve\n",
      "0s - loss: 0.4808 - acc: 0.9902 - val_loss: 0.6559 - val_acc: 0.9131\n",
      "Epoch 27/100\n",
      "Epoch 00026: val_acc did not improve\n",
      "0s - loss: 0.4463 - acc: 0.9915 - val_loss: 0.6406 - val_acc: 0.9040\n",
      "Epoch 28/100\n",
      "Epoch 00027: val_acc did not improve\n",
      "0s - loss: 0.4206 - acc: 0.9895 - val_loss: 0.6110 - val_acc: 0.9101\n",
      "Epoch 29/100\n",
      "Epoch 00028: val_acc did not improve\n",
      "0s - loss: 0.3972 - acc: 0.9915 - val_loss: 0.5855 - val_acc: 0.9116\n",
      "Epoch 30/100\n",
      "Epoch 00029: val_acc did not improve\n",
      "0s - loss: 0.3717 - acc: 0.9915 - val_loss: 0.5840 - val_acc: 0.8963\n",
      "Epoch 31/100\n",
      "Epoch 00030: val_acc did not improve\n",
      "0s - loss: 0.3456 - acc: 0.9921 - val_loss: 0.5624 - val_acc: 0.9055\n",
      "Epoch 32/100\n",
      "Epoch 00031: val_acc did not improve\n",
      "0s - loss: 0.3215 - acc: 0.9961 - val_loss: 0.5296 - val_acc: 0.9131\n",
      "Epoch 33/100\n",
      "Epoch 00032: val_acc did not improve\n",
      "0s - loss: 0.3078 - acc: 0.9935 - val_loss: 0.5190 - val_acc: 0.9055\n",
      "Epoch 34/100\n",
      "Epoch 00033: val_acc did not improve\n",
      "0s - loss: 0.2858 - acc: 0.9928 - val_loss: 0.5102 - val_acc: 0.9009\n",
      "Epoch 35/100\n",
      "Epoch 00034: val_acc did not improve\n",
      "0s - loss: 0.2687 - acc: 0.9961 - val_loss: 0.4710 - val_acc: 0.9085\n",
      "Epoch 36/100\n",
      "Epoch 00035: val_acc did not improve\n",
      "0s - loss: 0.2486 - acc: 0.9974 - val_loss: 0.4645 - val_acc: 0.9040\n",
      "Epoch 37/100\n",
      "Epoch 00036: val_acc did not improve\n",
      "0s - loss: 0.2372 - acc: 0.9961 - val_loss: 0.4743 - val_acc: 0.8963\n",
      "Epoch 00036: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fit aspect model using pretrained word vectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set the number of output units\n",
    "NUM_OUTPUT_UNITS=len(mlb1.classes_)\n",
    "\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# We used 400 labeled aspect reviews\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "MAX_DOC_LEN=30\n",
    "sequences_aspect = tokenizer.texts_to_sequences(aspect_l[\"Reviews\"])\n",
    "padded_sequences_aspect = pad_sequences(sequences_aspect, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "#\n",
    "\n",
    "#\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                padded_sequences_aspect[0:546], Y1[0:546], \\\n",
    "                test_size=0.3, random_state=0)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, NUM_OUTPUT_UNITS, \\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint = ModelCheckpoint(BEST_MODEL_FILEPATH1, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training=model.fit(X_train, Y_train, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping, checkpoint],\\\n",
    "          validation_data=[X_test, Y_test], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 96.84%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of aspect CNN model\n",
    "scores_1 = model.evaluate(padded_sequences_aspect, Y1, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores_1[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 287 samples, validate on 123 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_acc improved from -inf to 0.62195, saving model to best_model2\n",
      "0s - loss: 2.8678 - acc: 0.5418 - val_loss: 2.5441 - val_acc: 0.6220\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_acc did not improve\n",
      "0s - loss: 2.5571 - acc: 0.6481 - val_loss: 2.5231 - val_acc: 0.5894\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_acc improved from 0.62195 to 0.65041, saving model to best_model2\n",
      "0s - loss: 2.4674 - acc: 0.6794 - val_loss: 2.3825 - val_acc: 0.6504\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_acc did not improve\n",
      "0s - loss: 2.3259 - acc: 0.6934 - val_loss: 2.3155 - val_acc: 0.6301\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_acc improved from 0.65041 to 0.67886, saving model to best_model2\n",
      "0s - loss: 2.1631 - acc: 0.7822 - val_loss: 2.2354 - val_acc: 0.6789\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_acc did not improve\n",
      "0s - loss: 2.0263 - acc: 0.8240 - val_loss: 2.1632 - val_acc: 0.6626\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_acc improved from 0.67886 to 0.69512, saving model to best_model2\n",
      "0s - loss: 1.9247 - acc: 0.8432 - val_loss: 2.0913 - val_acc: 0.6951\n",
      "Epoch 8/100\n",
      "Epoch 00007: val_acc did not improve\n",
      "0s - loss: 1.8037 - acc: 0.8798 - val_loss: 2.0353 - val_acc: 0.6911\n",
      "Epoch 9/100\n",
      "Epoch 00008: val_acc improved from 0.69512 to 0.69919, saving model to best_model2\n",
      "0s - loss: 1.7234 - acc: 0.8885 - val_loss: 1.9951 - val_acc: 0.6992\n",
      "Epoch 10/100\n",
      "Epoch 00009: val_acc improved from 0.69919 to 0.71545, saving model to best_model2\n",
      "0s - loss: 1.6097 - acc: 0.9216 - val_loss: 1.9470 - val_acc: 0.7154\n",
      "Epoch 11/100\n",
      "Epoch 00010: val_acc did not improve\n",
      "0s - loss: 1.5456 - acc: 0.9251 - val_loss: 1.8978 - val_acc: 0.6992\n",
      "Epoch 12/100\n",
      "Epoch 00011: val_acc did not improve\n",
      "0s - loss: 1.4774 - acc: 0.9321 - val_loss: 1.8559 - val_acc: 0.7114\n",
      "Epoch 13/100\n",
      "Epoch 00012: val_acc improved from 0.71545 to 0.73984, saving model to best_model2\n",
      "0s - loss: 1.3711 - acc: 0.9460 - val_loss: 1.8540 - val_acc: 0.7398\n",
      "Epoch 14/100\n",
      "Epoch 00013: val_acc did not improve\n",
      "0s - loss: 1.3414 - acc: 0.9477 - val_loss: 1.7701 - val_acc: 0.7154\n",
      "Epoch 15/100\n",
      "Epoch 00014: val_acc did not improve\n",
      "0s - loss: 1.2648 - acc: 0.9512 - val_loss: 1.7221 - val_acc: 0.7154\n",
      "Epoch 16/100\n",
      "Epoch 00015: val_acc did not improve\n",
      "0s - loss: 1.1793 - acc: 0.9774 - val_loss: 1.7603 - val_acc: 0.7358\n",
      "Epoch 00015: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Fit sentiment model using pretrained word vectors\n",
    "\n",
    "# set the number of output units\n",
    "# as the number of classes\n",
    "from sklearn.model_selection import train_test_split\n",
    "NUM_OUTPUT_UNITS_1=len(mlb2.classes_)\n",
    "\n",
    "FILTER_SIZES=[2,3,4]\n",
    "\n",
    "BTACH_SIZE = 64\n",
    "NUM_EPOCHES = 100\n",
    "\n",
    "# With well trained word vectors, sample size can be reduced\n",
    "# We use our 450 labeled sentiment reviews\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "MAX_DOC_LEN=30\n",
    "sequences_sentiment = tokenizer.texts_to_sequences(sentiment_l[\"Reviews\"])\n",
    "padded_sequences_sentiment = pad_sequences(sequences_sentiment, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')\n",
    "\n",
    "#sequences_2 = tokenizer.texts_to_sequences(predict[1])\n",
    "#padded_sequences_2 = pad_sequences(sequences_2, \\\n",
    "                                 #maxlen=MAX_DOC_LEN, \\\n",
    "                                 #padding='post', \\\n",
    "                                 #truncating='post')\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(\\\n",
    "                padded_sequences_sentiment[0:410], Y2[0:410], \\\n",
    "                test_size=0.3, random_state=0)\n",
    "\n",
    "# create the model with embedding matrix\n",
    "model_1=cnn_model(FILTER_SIZES, MAX_NB_WORDS, \\\n",
    "                MAX_DOC_LEN, NUM_OUTPUT_UNITS_1, \\\n",
    "                PRETRAINED_WORD_VECTOR=embedding_matrix)\n",
    "\n",
    "earlyStopping_1=EarlyStopping(monitor='val_loss', patience=0, verbose=2, mode='min')\n",
    "checkpoint_1 = ModelCheckpoint(BEST_MODEL_FILEPATH2, monitor='val_acc', \\\n",
    "                             verbose=2, save_best_only=True, mode='max')\n",
    "    \n",
    "training_1=model_1.fit(X_train_1, Y_train_1, \\\n",
    "          batch_size=BTACH_SIZE, epochs=NUM_EPOCHES, \\\n",
    "          callbacks=[earlyStopping_1, checkpoint_1],\\\n",
    "          validation_data=[X_test_1, Y_test_1], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 91.47%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of sentiment CNN model\n",
    "scores_2 = model_1.evaluate(padded_sequences_sentiment, Y2, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model_1.metrics_names[1], scores_2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad reviews for prediction\n",
    "sequences_1 = tokenizer.texts_to_sequences(predict[\"reviews\"])\n",
    "padded_sequences_1 = pad_sequences(sequences_1, \\\n",
    "                                 maxlen=MAX_DOC_LEN, \\\n",
    "                                 padding='post', \\\n",
    "                                 truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model configuration of aspect model\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 30, 200)       1200200     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                  (None, 29, 64)        25664       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                  (None, 28, 64)        38464       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_4 (Conv1D)                  (None, 27, 64)        51264       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_2 (MaxPooling1D)             (None, 1, 64)         0           conv_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_3 (MaxPooling1D)             (None, 1, 64)         0           conv_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_4 (MaxPooling1D)             (None, 1, 64)         0           conv_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flat_2 (Flatten)                 (None, 64)            0           max_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_3 (Flatten)                 (None, 64)            0           max_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_4 (Flatten)                 (None, 64)            0           max_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concate (Concatenate)            (None, 192)           0           flat_2[0][0]                     \n",
      "                                                                   flat_3[0][0]                     \n",
      "                                                                   flat_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 192)           0           concate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 192)           37056       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 4)             772         dense[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,353,420\n",
      "Trainable params: 153,220\n",
      "Non-trainable params: 1,200,200\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "model configuration of sentiment model\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "main_input (InputLayer)          (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding (Embedding)            (None, 30, 200)       1200200     main_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv_2 (Conv1D)                  (None, 29, 64)        25664       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_3 (Conv1D)                  (None, 28, 64)        38464       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv_4 (Conv1D)                  (None, 27, 64)        51264       embedding[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_2 (MaxPooling1D)             (None, 1, 64)         0           conv_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_3 (MaxPooling1D)             (None, 1, 64)         0           conv_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "max_4 (MaxPooling1D)             (None, 1, 64)         0           conv_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flat_2 (Flatten)                 (None, 64)            0           max_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_3 (Flatten)                 (None, 64)            0           max_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flat_4 (Flatten)                 (None, 64)            0           max_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "concate (Concatenate)            (None, 192)           0           flat_2[0][0]                     \n",
      "                                                                   flat_3[0][0]                     \n",
      "                                                                   flat_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout (Dropout)                (None, 192)           0           concate[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 192)           37056       dropout[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "output (Dense)                   (None, 2)             386         dense[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 1,353,034\n",
      "Trainable params: 152,834\n",
      "Non-trainable params: 1,200,200\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# check model configuration\n",
    "print(\"model configuration of aspect model\")\n",
    "print(model.summary())\n",
    "print(\"\\n\")\n",
    "print(\"model configuration of sentiment model\")\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   ambience       0.72      0.62      0.67        21\n",
      "       food       0.83      0.89      0.86        95\n",
      "      price       0.85      0.61      0.71        18\n",
      "    service       0.89      0.55      0.68        31\n",
      "\n",
      "avg / total       0.83      0.76      0.79       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation of aspect CNN\n",
    "# Let's use samples[0:400]\n",
    "# as an evaluation set\n",
    "from sklearn.metrics import classification_report\n",
    "pred_1=model.predict(X_test)\n",
    "\n",
    "Y_pred_1=np.matrix(pred_1)\n",
    "Y_pred_1=np.where(Y_pred_1>0.3,1,0)\n",
    "#Y1=np.array(Y1)\n",
    "\n",
    "#Y_pred_1[0:10]\n",
    "#Y1[100:110]\n",
    "\n",
    "#y = map(lambda x: int(x), y)\n",
    "#answer = map(lambda x: int(x), answer)\n",
    "print(classification_report(Y_test, Y_pred_1, target_names=mlb1.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negative       0.76      0.50      0.60        50\n",
      "   Positive       0.73      0.89      0.80        73\n",
      "\n",
      "avg / total       0.74      0.73      0.72       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation of sentiment CNN\n",
    "# Let's use samples[0:400]\n",
    "# as an evaluation set\n",
    "from sklearn.metrics import classification_report\n",
    "pred_2=model_1.predict(X_test_1)\n",
    "\n",
    "Y_pred_2=np.matrix(pred_2)\n",
    "Y_pred_2=np.where(Y_pred_2>0.5,1,0)\n",
    "#Y=np.array(Y)\n",
    "\n",
    "#Y_pred_2[0:10]\n",
    "#Y2[100:110]\n",
    "\n",
    "#y = map(lambda x: int(x), y)\n",
    "#answer = map(lambda x: int(x), answer)\n",
    "print(classification_report(Y_test_1, Y_pred_2, target_names=mlb2.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (2, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (14, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (18, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (33, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (38, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (49, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (55, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (56, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (61, array([0, 0, 1, 0]), array([0, 0, 0, 0])), (64, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (67, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (80, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (88, array([0, 0, 0, 1]), array([0, 0, 0, 0])), (111, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (112, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (140, array([1, 0, 1, 0]), array([0, 0, 0, 0])), (149, array([1, 0, 0, 0]), array([0, 0, 0, 0])), (155, array([0, 1, 0, 0]), array([0, 0, 0, 0])), (159, array([1, 0, 0, 0]), array([0, 0, 0, 0]))]\n"
     ]
    }
   ],
   "source": [
    "errors_a=[]\n",
    "for idx, row in enumerate(Y_test):\n",
    "    if set(row) != set(Y_pred_1[idx]):\n",
    "        errors_a.append((idx, row, Y_pred_1[idx]))\n",
    "print(errors_a)\n",
    "\n",
    "#[i for i, j in zip(a, b) if i == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(73, array([1, 0]), array([0, 0]))]\n"
     ]
    }
   ],
   "source": [
    "errors=[]\n",
    "for idx, row in enumerate(Y_test_1):\n",
    "    if set(row) != set(Y_pred_2[idx]):\n",
    "        errors.append((idx, row, Y_pred_2[idx]))\n",
    "print(errors)\n",
    "\n",
    "#[i for i, j in zip(a, b) if i == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ambience' 'food' 'price' 'service']\n",
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [1 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 1 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [1 1 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 1 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#Predict the unlabelled dataset with aspects, and save the result into a csv file\n",
    "pred_11=model.predict(padded_sequences_1[0:])\n",
    "\n",
    "#we chose a threshold of 0.3, because at 30% probability, every review can have a label, and some aspects that \n",
    "#could be mentioned but not mentioned a lot in the review could be reflected\n",
    "Y_pred_11=np.matrix(pred_11)\n",
    "Y_pred_11=np.where(Y_pred_11>0.3,1,0)\n",
    "\n",
    "#name of each column\n",
    "print(mlb1.classes_)\n",
    "print(Y_pred_11[0:200])\n",
    "\n",
    "result_1 = pd.DataFrame(Y_pred_11)\n",
    "result_1.to_csv('output1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Positive']\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " ..., \n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#Predict the unlabelled dataset with sentiments, and save the result into a csv file\n",
    "pred_12=model_1.predict(padded_sequences_1[0:])\n",
    "Y_pred_12=np.matrix(pred_12)\n",
    "#print(Y_pred_12)\n",
    "\n",
    "#Because each review can only have a sentiment as a whole, we chose sentiment with the highest probability for each reveiw \n",
    "#as its representitive sentiment. And the highest probability is converted to 1, others 0.\n",
    "print(mlb2.classes_)\n",
    "print((Y_pred_12 == Y_pred_12.max(axis=1)).astype(float))\n",
    "\n",
    "result_2 = pd.DataFrame((Y_pred_12 == Y_pred_12.max(axis=1)).astype(float))\n",
    "result_2.to_csv('output2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
